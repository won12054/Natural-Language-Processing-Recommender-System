{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "18be3a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nlpaug.augmenter.word as naw\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5474a9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>RT @ashish_vaidya1: @Bhupendrapbjp sir, keshod mamlatdar office says they didnt receive grant for covid19 ex-gratia claim from govt. Many c…</td>\n",
       "      <td>ashish_vaidya1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>RT @TheAngryEpi: Informally reached out to a few epidemiologists about an open position in Arizona. Their response: “I would not work in AZ…</td>\n",
       "      <td>greensnow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>RT @sinovic: If this is a joke from the @DMRegister it really isn’t funny. If it’s serious, they are completely out of touch from reality w…</td>\n",
       "      <td>RadioBradshaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>RT @MicahPollak: Well, #COVID19 is once again the leading cause of death in #Indiana (based on average daily deaths) and closing in on (aga…</td>\n",
       "      <td>KristinaTraxler</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment  \\\n",
       "0  positive   \n",
       "1  negative   \n",
       "2  positive   \n",
       "3  negative   \n",
       "\n",
       "                                                                                                                                           text  \\\n",
       "0  RT @ashish_vaidya1: @Bhupendrapbjp sir, keshod mamlatdar office says they didnt receive grant for covid19 ex-gratia claim from govt. Many c…   \n",
       "1  RT @TheAngryEpi: Informally reached out to a few epidemiologists about an open position in Arizona. Their response: “I would not work in AZ…   \n",
       "2  RT @sinovic: If this is a joke from the @DMRegister it really isn’t funny. If it’s serious, they are completely out of touch from reality w…   \n",
       "3  RT @MicahPollak: Well, #COVID19 is once again the leading cause of death in #Indiana (based on average daily deaths) and closing in on (aga…   \n",
       "\n",
       "              user  \n",
       "0   ashish_vaidya1  \n",
       "1        greensnow  \n",
       "2    RadioBradshaw  \n",
       "3  KristinaTraxler  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Preprocessing\n",
    "\n",
    "a. Load the data into a dataframe. \n",
    "    Name it \"jungyu_df\", and examine the data. \n",
    "    You will notice that the file has a header and four tweets with their sentiments.\n",
    "'''\n",
    "\n",
    "filepath = 'C:/Users/Public/6th/NLP and Recommender Systems/Assignment1/COVID19_mini.csv'\n",
    "\n",
    "jungyu_df = pd.read_csv(filepath)\n",
    "jungyu_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "459e44df",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "b. Drop the user column.\n",
    "'''\n",
    "jungyu_df = jungyu_df.drop(columns=['user'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f8573816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 1:\n",
      "RT @ashish_vaidya1: @Bhupendrapbjp sir, keshod mamlatdar office says they didnt receive grant for covid19 ex-gratia claim from govt. Many c…\n",
      "\n",
      "Tweet 2:\n",
      "RT @TheAngryEpi: Informally reached out to a few epidemiologists about an open position in Arizona. Their response: “I would not work in AZ…\n",
      "\n",
      "Tweet 3:\n",
      "RT @sinovic: If this is a joke from the @DMRegister it really isn’t funny. If it’s serious, they are completely out of touch from reality w…\n",
      "\n",
      "Tweet 4:\n",
      "RT @MicahPollak: Well, #COVID19 is once again the leading cause of death in #Indiana (based on average daily deaths) and closing in on (aga…\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "c. Use regular expressions or python string methods to get rid of the additional data at the begging and end of each tweet.\n",
    "d. Check the tweet data and identify, if you need to carry out any further pre-processing steps, you should at least do two or three more steps. \n",
    "'''\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "for i, text in enumerate(jungyu_df['text']):\n",
    "    print(f'Tweet {i+1}:\\n{text}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "65101282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "    # Remove \"RT @username: \" at the beginning\n",
    "    text = re.sub(r'^RT @\\w+: ', '', text)\n",
    "    # Remove all other @username mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove trailing ellipsis\n",
    "    text = re.sub(r'…$', '', text)\n",
    "    # Remove any characters that are not letters, numbers, or spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove any leading or trailing whitespace\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8026337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jungyu_df['text'] = jungyu_df['text'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c9b6f810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 1:\n",
      "sir keshod mamlatdar office says they didnt receive grant for covid19 exgratia claim from govt Many c\n",
      "\n",
      "Tweet 2:\n",
      "Informally reached out to a few epidemiologists about an open position in Arizona Their response I would not work in AZ\n",
      "\n",
      "Tweet 3:\n",
      "If this is a joke from the it really isnt funny If its serious they are completely out of touch from reality w\n",
      "\n",
      "Tweet 4:\n",
      "Well COVID19 is once again the leading cause of death in Indiana based on average daily deaths and closing in on aga\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(jungyu_df['text']):\n",
    "    print(f'Tweet {i+1}:\\n{text}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ae211c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data Augmentation\n",
    "a. Tokenize the cleaned tweet.\n",
    "'''\n",
    "def tokenize_tweet(text):\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "40d8965f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 1 tokens:\n",
      "['sir', 'keshod', 'mamlatdar', 'office', 'says', 'they', 'didnt', 'receive', 'grant', 'for', 'covid19', 'exgratia', 'claim', 'from', 'govt', 'Many', 'c']\n",
      "\n",
      "Tweet 2 tokens:\n",
      "['Informally', 'reached', 'out', 'to', 'a', 'few', 'epidemiologists', 'about', 'an', 'open', 'position', 'in', 'Arizona', 'Their', 'response', 'I', 'would', 'not', 'work', 'in', 'AZ']\n",
      "\n",
      "Tweet 3 tokens:\n",
      "['If', 'this', 'is', 'a', 'joke', 'from', 'the', 'it', 'really', 'isnt', 'funny', 'If', 'its', 'serious', 'they', 'are', 'completely', 'out', 'of', 'touch', 'from', 'reality', 'w']\n",
      "\n",
      "Tweet 4 tokens:\n",
      "['Well', 'COVID19', 'is', 'once', 'again', 'the', 'leading', 'cause', 'of', 'death', 'in', 'Indiana', 'based', 'on', 'average', 'daily', 'deaths', 'and', 'closing', 'in', 'on', 'aga']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jungyu_df['tokens'] = jungyu_df['text'].apply(tokenize_tweet)\n",
    "\n",
    "for i, tokens in enumerate(jungyu_df['tokens']):\n",
    "    print(f'Tweet {i+1} tokens:\\n{tokens}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "98e55c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "b. Remove stop words if you haven't done so earlier, be careful\n",
    "'''\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "62980d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 1 filtered tokens:\n",
      "['sir', 'keshod', 'mamlatdar', 'office', 'says', 'didnt', 'receive', 'grant', 'covid19', 'exgratia', 'claim', 'govt', 'Many']\n",
      "\n",
      "Tweet 2 filtered tokens:\n",
      "['Informally', 'reached', 'epidemiologists', 'open', 'position', 'Arizona', 'response', 'would', 'work', 'AZ']\n",
      "\n",
      "Tweet 3 filtered tokens:\n",
      "['joke', 'really', 'isnt', 'funny', 'serious', 'completely', 'touch', 'reality']\n",
      "\n",
      "Tweet 4 filtered tokens:\n",
      "['Well', 'COVID19', 'leading', 'cause', 'death', 'Indiana', 'based', 'average', 'daily', 'deaths', 'closing', 'aga']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_stop_words(tokens):\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words and len(word) > 1]\n",
    "    return filtered_tokens\n",
    "\n",
    "jungyu_df['filtered_tokens'] = jungyu_df['tokens'].apply(remove_stop_words)\n",
    "\n",
    "for i, tokens in enumerate(jungyu_df['filtered_tokens']):\n",
    "    print(f'Tweet {i+1} filtered tokens:\\n{tokens}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "30a58ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "c. Per tweet choose four words randomly. \n",
    "d. Get synonyms of each of the words selected in step iii\n",
    "e. Select the most similar synonym, and replace the original word with the synonym to create a new tweet (You should not replace the original tweet, you need to add a new copy of each tweet using the selected synonyms to your dataframe as a row and maintain the original sentiment\n",
    "'''\n",
    "word2vec_model_path = 'C:/Users/Public/6th/NLP and Recommender Systems/Assignment1/GoogleNews-vectors-negative300.bin'\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ec32e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.WordEmbsAug(\n",
    "    model_type='word2vec',\n",
    "    model_path=word2vec_model_path,\n",
    "    action='substitute',\n",
    "    aug_max=4,\n",
    "    stopwords=stop_words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ca3166c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_tweets = []\n",
    "\n",
    "for i, row in jungyu_df.iterrows():\n",
    "    original_tweet = ' '.join(row['filtered_tokens']) \n",
    "    \n",
    "    augmented_tweet = aug.augment(original_tweet)\n",
    "    \n",
    "    augmented_tweet = ''.join(augmented_tweet)\n",
    "    \n",
    "    augmented_tweets.append({'sentiment': row['sentiment'], 'cleaned_text': augmented_tweet})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6e462f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>sir keshod mamlatdar office says didnt receive funding covid19 Rs1_lakh undisputed JK_govt Many</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>Informally cwalters@pioneerpress.com epidemiologists open position Arizona response promised work Terry_Goddard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>joke really isnt funny catastrophic similarly touch Unan1mous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>Hmmmmmm COVID19 world'slargest cause death Indiana USA_Subjex_Corporation average daily deaths closure aga</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment  \\\n",
       "0  positive   \n",
       "1  negative   \n",
       "2  positive   \n",
       "3  negative   \n",
       "\n",
       "                                                                                                      cleaned_text  \n",
       "0                  sir keshod mamlatdar office says didnt receive funding covid19 Rs1_lakh undisputed JK_govt Many  \n",
       "1  Informally cwalters@pioneerpress.com epidemiologists open position Arizona response promised work Terry_Goddard  \n",
       "2                                                    joke really isnt funny catastrophic similarly touch Unan1mous  \n",
       "3       Hmmmmmm COVID19 world'slargest cause death Indiana USA_Subjex_Corporation average daily deaths closure aga  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_df = pd.DataFrame(augmented_tweets)\n",
    "augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "271d2a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "jungyu_df['cleaned_text'] = jungyu_df['filtered_tokens'].apply(lambda tokens: ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "44727747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>sir keshod mamlatdar office says didnt receive grant covid19 exgratia claim govt Many</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>Informally reached epidemiologists open position Arizona response would work AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>joke really isnt funny serious completely touch reality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>Well COVID19 leading cause death Indiana based average daily deaths closing aga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>sir keshod mamlatdar office says didnt receive funding covid19 Rs1_lakh undisputed JK_govt Many</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>Informally cwalters@pioneerpress.com epidemiologists open position Arizona response promised work Terry_Goddard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>joke really isnt funny catastrophic similarly touch Unan1mous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>negative</td>\n",
       "      <td>Hmmmmmm COVID19 world'slargest cause death Indiana USA_Subjex_Corporation average daily deaths closure aga</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment  \\\n",
       "0  positive   \n",
       "1  negative   \n",
       "2  positive   \n",
       "3  negative   \n",
       "4  positive   \n",
       "5  negative   \n",
       "6  positive   \n",
       "7  negative   \n",
       "\n",
       "                                                                                                      cleaned_text  \n",
       "0                            sir keshod mamlatdar office says didnt receive grant covid19 exgratia claim govt Many  \n",
       "1                                  Informally reached epidemiologists open position Arizona response would work AZ  \n",
       "2                                                          joke really isnt funny serious completely touch reality  \n",
       "3                                  Well COVID19 leading cause death Indiana based average daily deaths closing aga  \n",
       "4                  sir keshod mamlatdar office says didnt receive funding covid19 Rs1_lakh undisputed JK_govt Many  \n",
       "5  Informally cwalters@pioneerpress.com epidemiologists open position Arizona response promised work Terry_Goddard  \n",
       "6                                                    joke really isnt funny catastrophic similarly touch Unan1mous  \n",
       "7       Hmmmmmm COVID19 world'slargest cause death Indiana USA_Subjex_Corporation average daily deaths closure aga  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jungyu_df_concat = pd.concat([jungyu_df[['sentiment', 'cleaned_text']], augmented_df], ignore_index=True)\n",
    "jungyu_df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "82e8dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filepath = 'C:/Users/Public/6th/NLP and Recommender Systems/Assignment1/jungyu_df_after_random_insertion.txt'\n",
    "jungyu_df_concat.to_csv(output_filepath, index=False, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
